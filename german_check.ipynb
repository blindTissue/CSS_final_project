{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-02T22:37:56.950066Z",
     "start_time": "2024-05-02T22:37:54.511179Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "dataset = load_dataset(\"wmt/wmt16\", 'de-en')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T22:37:58.866410Z",
     "start_time": "2024-05-02T22:37:58.864116Z"
    }
   },
   "cell_type": "code",
   "source": "print(dataset)",
   "id": "b4715de7b8ade9e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 4548885\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2169\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2999\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T22:38:04.001603Z",
     "start_time": "2024-05-02T22:38:03.561001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n"
   ],
   "id": "ad5967b8eaebcc25",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sungwonkim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T22:38:07.878425Z",
     "start_time": "2024-05-02T22:38:07.876410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ],
   "id": "52a5f619f59d0f6e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T22:38:21.333487Z",
     "start_time": "2024-05-02T22:38:10.166144Z"
    }
   },
   "cell_type": "code",
   "source": "a = word_tokenize(dataset['train']['translation'][0]['de'], language='german')",
   "id": "6abb67c5ab15730c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T17:54:20.550280Z",
     "start_time": "2024-05-02T17:54:20.547084Z"
    }
   },
   "cell_type": "code",
   "source": "a",
   "id": "e061dad62defbf73",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wiederaufnahme', 'der', 'Sitzungsperiode']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T22:39:46.524395Z",
     "start_time": "2024-05-02T22:39:46.521077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get rid of stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('german'))\n",
    "filtered_sentence = [word for word in a if word not in stop_words]\n",
    "filtered_sentence"
   ],
   "id": "8238885a48b3bfe1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wiederaufnahme', 'Sitzungsperiode']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T22:39:47.029108Z",
     "start_time": "2024-05-02T22:39:47.026829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get rid of punctuation\n",
    "import string\n",
    "filtered_sentence = [word for word in filtered_sentence if word not in string.punctuation]\n",
    "filtered_sentence"
   ],
   "id": "dc37a9145ebb67fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wiederaufnahme', 'Sitzungsperiode']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T22:46:35.793432Z",
     "start_time": "2024-05-02T22:46:06.492472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "english_sentences = []\n",
    "german_sentences = []\n",
    "english_stop_words = set(stopwords.words('english'))\n",
    "german_stop_words = set(stopwords.words('german'))\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "for item in tqdm(dataset['train']['translation'][:100000]):\n",
    "    en_tokenized = word_tokenize(item['en'])\n",
    "    #de_tokenized = word_tokenize(item['de'], language='german')\n",
    "    #de_tokenized = [word.lemma_ for word in nlp(item['de'])]\n",
    "    de_tokenized = [lemma for (word,lemma,pos) in tagger.tag_sent(item['de'].split())]\n",
    "    #lowercase\n",
    "    en_tokenized = [word.lower() for word in en_tokenized]\n",
    "    de_tokenized = [word.lower() for word in de_tokenized]\n",
    "    #de_tokenized = [word.lemma_ for word in nlp(de_tokenized)]\n",
    "    \n",
    "    en_filtered = [word for word in en_tokenized if word not in english_stop_words]\n",
    "    en_filtered = [word for word in en_filtered if word not in string.punctuation]\n",
    "    de_filtered = [word for word in de_tokenized if word not in german_stop_words]\n",
    "    de_filtered = [word for word in de_filtered if word not in string.punctuation]\n",
    "    \n",
    "    english_sentences.append(en_filtered)\n",
    "    german_sentences.append(de_filtered)"
   ],
   "id": "7ef308a895663f60",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4444/100000 [00:20<07:24, 214.99it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m en_tokenized \u001B[38;5;241m=\u001B[39m word_tokenize(item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m#de_tokenized = word_tokenize(item['de'], language='german')\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m#de_tokenized = [word.lemma_ for word in nlp(item['de'])]\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m de_tokenized \u001B[38;5;241m=\u001B[39m [lemma \u001B[38;5;28;01mfor\u001B[39;00m (word,lemma,pos) \u001B[38;5;129;01min\u001B[39;00m \u001B[43mtagger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtag_sent\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mde\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m]\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m#lowercase\u001B[39;00m\n\u001B[1;32m     15\u001B[0m en_tokenized \u001B[38;5;241m=\u001B[39m [word\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m en_tokenized]\n",
      "File \u001B[0;32m~/PycharmProjects/CSS_final_project/.venv/lib/python3.11/site-packages/HanTa/HanoverTagger.py:531\u001B[0m, in \u001B[0;36mHanoverTagger.tag_sent\u001B[0;34m(self, sent, taglevel, casesensitive)\u001B[0m\n\u001B[1;32m    529\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mint2tag[t] \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m tags]\n\u001B[1;32m    530\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m taglevel \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 531\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m[\u001B[49m\u001B[43m(\u001B[49m\u001B[43msent\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_analyze\u001B[49m\u001B[43m(\u001B[49m\u001B[43msent\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtaglevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mint2tag\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtags\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msent\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    532\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m taglevel \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m    533\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [(sent[i], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_analyze(sent[i], tags[i], taglevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mint2tag[tags[i]]) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(sent))]\n",
      "File \u001B[0;32m~/PycharmProjects/CSS_final_project/.venv/lib/python3.11/site-packages/HanTa/HanoverTagger.py:531\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    529\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mint2tag[t] \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m tags]\n\u001B[1;32m    530\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m taglevel \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 531\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [(sent[i], \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_analyze\u001B[49m\u001B[43m(\u001B[49m\u001B[43msent\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtaglevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mint2tag[tags[i]]) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(sent))]\n\u001B[1;32m    532\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m taglevel \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m    533\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [(sent[i], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_analyze(sent[i], tags[i], taglevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mint2tag[tags[i]]) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(sent))]\n",
      "File \u001B[0;32m~/PycharmProjects/CSS_final_project/.venv/lib/python3.11/site-packages/HanTa/HanoverTagger.py:428\u001B[0m, in \u001B[0;36mHanoverTagger._analyze\u001B[0;34m(self, word, pos, taglevel)\u001B[0m\n\u001B[1;32m    426\u001B[0m     morphemes \u001B[38;5;241m=\u001B[39m [(pos,wlen),(postag,wlen\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)]\n\u001B[1;32m    427\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:        \n\u001B[0;32m--> 428\u001B[0m     morphemes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43manalyze_viterbi\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostag\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    430\u001B[0m postag \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mmorphemes[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    431\u001B[0m str_postag  \u001B[38;5;241m=\u001B[39m  \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mint2tag[postag] \n",
      "File \u001B[0;32m~/PycharmProjects/CSS_final_project/.venv/lib/python3.11/site-packages/HanTa/HanoverTagger.py:250\u001B[0m, in \u001B[0;36mHanoverTagger.analyze_viterbi\u001B[0;34m(self, word, targetpos)\u001B[0m\n\u001B[1;32m    248\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m    249\u001B[0m followup \u001B[38;5;241m=\u001B[39m [(t,p) \u001B[38;5;28;01mfor\u001B[39;00m (t,p) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlp_trans(state, final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;28;01mif\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m reachable_t]\n\u001B[0;32m--> 250\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, wlen \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m    251\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m tag1, lpt \u001B[38;5;129;01min\u001B[39;00m followup:\n\u001B[1;32m    252\u001B[0m         \u001B[38;5;66;03m#lp1 = self.lp_m_t(word[i:j], tag1)\u001B[39;00m\n\u001B[1;32m    253\u001B[0m         \u001B[38;5;66;03m#if not localstrict and lp1 == -math.inf:\u001B[39;00m\n\u001B[1;32m    254\u001B[0m         \u001B[38;5;66;03m#    lp1 = self.guess_lp_m_t(word[i:j], tag1,wlen)\u001B[39;00m\n\u001B[1;32m    255\u001B[0m         \u001B[38;5;66;03m#lp1 += lp + lpt\u001B[39;00m\n\u001B[1;32m    256\u001B[0m         lp1 \u001B[38;5;241m=\u001B[39m lp \u001B[38;5;241m+\u001B[39m lpt \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlp_m_t(word[i:j], tag1, wlen,localstrict)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T22:49:32.441632Z",
     "start_time": "2024-05-02T22:49:31.919436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    " "
   ],
   "id": "f6816a7657079457",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T22:46:40.122350Z",
     "start_time": "2024-05-02T22:46:40.119888Z"
    }
   },
   "cell_type": "code",
   "source": "de_tokenized",
   "id": "140fb4a0b972fb62",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mein',\n",
       " 'erachten',\n",
       " 'können',\n",
       " 'der',\n",
       " 'portugiesisch',\n",
       " 'präsidentschaft',\n",
       " 'in',\n",
       " 'positiv',\n",
       " 'weise',\n",
       " 'dazu',\n",
       " 'beitragen.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T22:40:38.138254Z",
     "start_time": "2024-05-02T22:40:38.090995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from HanTa import HanoverTagger as ht\n",
    "\n",
    "tagger = ht.HanoverTagger('morphmodel_ger.pgz')"
   ],
   "id": "8ad844c492c86c85",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T22:41:32.603031Z",
     "start_time": "2024-05-02T22:41:32.495247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from HanTa import HanoverTagger as ht\n",
    "\n",
    "tagger = ht.HanoverTagger('morphmodel_ger.pgz')\n",
    "\n",
    "mails=['Hallo. Ich spielte am frühen Morgen und ging dann zu einem Freund. Auf Wiedersehen',\n",
    "       'Guten Tag Ich mochte Bälle und will etwas kaufen. Tschüss']\n",
    "\n",
    "mails_lemma = []\n",
    "for mail in mails:\n",
    "    lemma = [lemma for (word,lemma,pos) in tagger.tag_sent(mail.split())]\n",
    "    mails_lemma.append(' '.join(lemma))"
   ],
   "id": "1f8709a6dcfca9c0",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T22:44:54.888663Z",
     "start_time": "2024-05-02T22:44:54.875761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in mails:\n",
    "    item = [word.lemma_ for word in nlp(i)]\n",
    "    print(item)\n"
   ],
   "id": "eccc3c7407c295a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hallo', '--', 'ich', 'spielen', 'an', 'früh', 'Morgen', 'und', 'gehen', 'dann', 'zu', 'ein', 'Freund', '--', 'auf', 'wiedersehen']\n",
      "['gut', 'Tag', 'ich', 'mögen', 'Ball', 'und', 'wollen', 'etwas', 'kaufen', '--', 'Tschüss']\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T22:41:40.250775Z",
     "start_time": "2024-05-02T22:41:40.247965Z"
    }
   },
   "cell_type": "code",
   "source": "mails_lemma",
   "id": "9058e51ee4c3b03b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hallo. ich spielen an früh Morgen und gehen dann zu ein Freund. auf Wiedersehen',\n",
       " 'gut Tag ich mögen Ball und wollen etwas Kaufen. Tschüss']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T22:40:48.028407Z",
     "start_time": "2024-05-02T22:40:48.021739Z"
    }
   },
   "cell_type": "code",
   "source": "tagger.tag_sent(\"Frau Präsidentin, zur Geschäftsordnung.\")",
   "id": "f228e35efeac2ebc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('F', 'f', 'XY'),\n",
       " ('r', 'r', 'XY'),\n",
       " ('a', 'a', 'XY'),\n",
       " ('u', 'u', 'XY'),\n",
       " (' ', ' ', 'XY'),\n",
       " ('P', 'p', 'XY'),\n",
       " ('r', 'r', 'XY'),\n",
       " ('ä', 'ä', 'XY'),\n",
       " ('s', 's', 'XY'),\n",
       " ('i', 'i', 'XY'),\n",
       " ('d', 'd', 'XY'),\n",
       " ('e', 'e', 'XY'),\n",
       " ('n', 'n', 'XY'),\n",
       " ('t', 'T', 'NE'),\n",
       " ('i', 'i', 'XY'),\n",
       " ('n', 'n', 'XY'),\n",
       " (',', ',', '$,'),\n",
       " (' ', ' ', 'XY'),\n",
       " ('z', 'Z', 'NN'),\n",
       " ('u', 'u', 'XY'),\n",
       " ('r', 'r', 'XY'),\n",
       " (' ', ' ', 'XY'),\n",
       " ('G', 'G', 'NE'),\n",
       " ('e', 'e', 'XY'),\n",
       " ('s', 's', 'XY'),\n",
       " ('c', 'c', 'XY'),\n",
       " ('h', 'h', 'XY'),\n",
       " ('ä', 'ä', 'XY'),\n",
       " ('f', 'f', 'XY'),\n",
       " ('t', 'T', 'FM'),\n",
       " ('s', 'S', 'FM'),\n",
       " ('o', 'O', 'NE'),\n",
       " ('r', 'r', 'XY'),\n",
       " ('d', 'd', 'XY'),\n",
       " ('n', 'n', 'XY'),\n",
       " ('u', 'u', 'XY'),\n",
       " ('n', 'n', 'XY'),\n",
       " ('g', 'G', 'NE'),\n",
       " ('.', '.', '$.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T22:40:10.620303Z",
     "start_time": "2024-05-02T22:40:10.620256Z"
    }
   },
   "cell_type": "code",
   "source": "german_sentences[1]",
   "id": "1f8bce9bab82d7eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T18:00:10.096810Z",
     "start_time": "2024-05-02T18:00:10.095071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(english_stop_words)\n",
    "# see if it includes 'the'\n",
    "\n",
    "print('the' in english_stop_words)"
   ],
   "id": "c0f245ae128f7553",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doesn', 'by', 'while', 'me', 'itself', 'mustn', 'were', 'ourselves', 've', 'them', \"you'd\", 'didn', 'do', 'don', 'hadn', 'there', 'once', 'under', 're', \"it's\", 'down', 'he', \"isn't\", 'yourself', 'any', 'just', 'own', 'aren', 't', 'what', 'my', 'no', 'are', 'your', 'll', 'wasn', 'most', 'is', 'than', 'm', 'her', 'same', 'into', 'had', 'needn', 'be', 'ours', 'only', \"wouldn't\", 'hasn', 'does', 'or', 'our', 'to', \"won't\", 'and', 'such', \"should've\", 'each', 'too', \"mightn't\", 'but', 'above', \"doesn't\", 'i', 'against', \"that'll\", 'having', 'all', \"you're\", 'won', 'couldn', 'his', 'where', 'why', 'before', 'during', 'until', 'some', 'shan', 'theirs', 'again', 'on', 'now', \"she's\", 'myself', 'yours', 'their', 'yourselves', 'when', \"needn't\", 'been', 'weren', 'herself', 'ma', 'few', 'this', 'then', 'over', 'doing', 'those', \"haven't\", 'it', 'not', 'these', 'that', 'am', 'as', 'y', 'nor', 'ain', 'wouldn', 'himself', \"you've\", 'about', 'at', 'from', 'a', 'the', 'who', 'because', \"mustn't\", 'd', 'after', 'did', 'off', 'very', 'hers', 'whom', 'she', 'they', 'so', \"you'll\", 'up', 'which', 'of', 'between', 'o', \"couldn't\", 'can', 'themselves', \"hasn't\", 'should', \"aren't\", 'was', 'both', \"hadn't\", 'for', 'shouldn', 'will', 'an', \"didn't\", \"don't\", 'we', 'how', 'more', 's', 'its', 'you', 'have', \"shouldn't\", 'has', 'through', 'isn', 'below', 'being', 'in', 'if', 'mightn', 'here', 'with', 'out', \"wasn't\", 'him', 'haven', \"shan't\", \"weren't\", 'other', 'further'}\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T17:57:22.108081Z",
     "start_time": "2024-05-02T17:57:21.701815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# make everything lowercase\n",
    "english_sentences = [[word.lower() for word in sentence] for sentence in english_sentences]\n",
    "german_sentences = [[word.lower() for word in sentence] for sentence in german_sentences]"
   ],
   "id": "2a13be05fc1e00b9",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T18:01:39.930517Z",
     "start_time": "2024-05-02T18:01:39.708146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get number of unique words in the dataset\n",
    "english_words = set()\n",
    "german_words = set()\n",
    "for sentence in english_sentences:\n",
    "    for word in sentence:\n",
    "        english_words.add(word)\n",
    "for sentence in german_sentences:\n",
    "    for word in sentence:\n",
    "        german_words.add(word)\n",
    "        \n",
    "print(\"Number of unique English words: \", len(english_words))\n",
    "print(\"Number of unique Czech words: \", len(german_words))"
   ],
   "id": "ca5b5ae96bba4916",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique English words:  32012\n",
      "Number of unique Czech words:  74376\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T18:17:49.459397Z",
     "start_time": "2024-05-02T18:17:49.328120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print top 50 most common words in german\n",
    "from collections import Counter\n",
    "german_words = [word for sentence in german_sentences for word in sentence]\n",
    "german_word_freq = Counter(german_words)\n",
    "    \n",
    "# print words only\n",
    "top_50 = german_word_freq.most_common(100)\n",
    "for word, freq in top_50:\n",
    "    print(word)"
   ],
   "id": "6e04c82fc8c90505",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kommission\n",
      "europäischen\n",
      "herr\n",
      "union\n",
      "möchte\n",
      "müssen\n",
      "``\n",
      "präsident\n",
      "parlament\n",
      "bericht\n",
      "muß\n",
      "europäische\n",
      "frau\n",
      "mitgliedstaaten\n",
      "wurde\n",
      "frage\n",
      "mehr\n",
      "rat\n",
      "jedoch\n",
      "europa\n",
      "gibt\n",
      "heute\n",
      "herrn\n",
      "bereits\n",
      "sowie\n",
      "maßnahmen\n",
      "ganz\n",
      "geht\n",
      "deshalb\n",
      "darauf\n",
      "entwicklung\n",
      "unserer\n",
      "denen\n",
      "parlaments\n",
      "dafür\n",
      "sollten\n",
      "richtlinie\n",
      "vorschlag\n",
      "kollegen\n",
      "immer\n",
      "sagen\n",
      "recht\n",
      "wurden\n",
      "rahmen\n",
      "insbesondere\n",
      "daher\n",
      "menschen\n",
      "natürlich\n",
      "fragen\n",
      "bürger\n",
      "bereich\n",
      "zeit\n",
      "darüber\n",
      "neuen\n",
      "schon\n",
      "länder\n",
      "jahren\n",
      "arbeit\n",
      "wäre\n",
      "zusammenarbeit\n",
      "eu\n",
      "problem\n",
      "unternehmen\n",
      "fraktion\n",
      "wichtig\n",
      "fall\n",
      "namen\n",
      "neue\n",
      "betrifft\n",
      "viele\n",
      "zwei\n",
      "jahr\n",
      "kommissar\n",
      "tun\n",
      "meinung\n",
      "dabei\n",
      "worden\n",
      "ja\n",
      "unterstützung\n",
      "politik\n",
      "lassen\n",
      "besteht\n",
      "zusammenhang\n",
      "ländern\n",
      "rates\n",
      "euro\n",
      "wissen\n",
      "gemeinsamen\n",
      "stellen\n",
      "nämlich\n",
      "geben\n",
      "thema\n",
      "präsidentin\n",
      "politischen\n",
      "mittel\n",
      "land\n",
      "regierung\n",
      "probleme\n",
      "abstimmung\n",
      "darf\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T18:01:49.059240Z",
     "start_time": "2024-05-02T18:01:48.929369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print top 50 most common words in english\n",
    "english_words = [word for sentence in english_sentences for word in sentence]\n",
    "english_word_freq = Counter(english_words)\n",
    "\n",
    "# print words only\n",
    "top_50 = english_word_freq.most_common(50)\n",
    "\n",
    "for word, freq in top_50:\n",
    "    print(word)"
   ],
   "id": "e95c92f69b1391d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "european\n",
      "mr\n",
      "commission\n",
      "would\n",
      "also\n",
      "must\n",
      "president\n",
      "parliament\n",
      "union\n",
      "report\n",
      "like\n",
      "states\n",
      "member\n",
      "council\n",
      "one\n",
      "countries\n",
      "europe\n",
      "policy\n",
      "need\n",
      "people\n",
      "us\n",
      "time\n",
      "new\n",
      "important\n",
      "therefore\n",
      "rights\n",
      "committee\n",
      "make\n",
      "take\n",
      "however\n",
      "way\n",
      "support\n",
      "fact\n",
      "work\n",
      "made\n",
      "first\n",
      "community\n",
      "question\n",
      "eu\n",
      "political\n",
      "issue\n",
      "commissioner\n",
      "social\n",
      "say\n",
      "point\n",
      "many\n",
      "within\n",
      "development\n",
      "debate\n",
      "directive\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T16:59:02.902101Z",
     "start_time": "2024-05-02T16:59:02.278562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "nlp = spacy.load('de_core_news_sm')"
   ],
   "id": "a4a89ce47a749101",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T17:05:33.005967Z",
     "start_time": "2024-05-02T17:05:32.676038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from spacy.lang.de.examples import sentences \n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "doc = nlp(sentences[0])\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ],
   "id": "3487ed5f8bd6a086",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die ganze Stadt ist ein Startup: Shenzhen ist das Silicon Valley für Hardware-Firmen\n",
      "Die DET nk\n",
      "ganze ADJ nk\n",
      "Stadt NOUN sb\n",
      "ist AUX ROOT\n",
      "ein DET nk\n",
      "Startup NOUN pd\n",
      ": PUNCT punct\n",
      "Shenzhen NOUN sb\n",
      "ist AUX cj\n",
      "das DET nk\n",
      "Silicon PROPN pnc\n",
      "Valley PROPN sb\n",
      "für ADP mnr\n",
      "Hardware-Firmen NOUN nk\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T22:52:17.498247Z",
     "start_time": "2024-05-02T22:52:17.266829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    " \n",
    "# Define a sample text\n",
    "text = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
    " \n",
    "# Process the text using spaCy\n",
    "doc = nlp(text)\n",
    " \n",
    "# Extract lemmatized tokens\n",
    "lemmatized_tokens = [token.lemma_ for token in doc]\n",
    " \n",
    "# Join the lemmatized tokens into a sentence\n",
    "lemmatized_text = ' '.join(lemmatized_tokens)\n",
    " \n",
    "# Print the original and lemmatized text\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Lemmatized Text:\", lemmatized_text)"
   ],
   "id": "bc38780367683cfe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The quick brown foxes are jumping over the lazy dogs.\n",
      "Lemmatized Text: the quick brown fox be jump over the lazy dog .\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T22:52:48.274822Z",
     "start_time": "2024-05-02T22:52:47.920369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "text = \"Frau Präsidentin, zur Geschäftsordnung.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Lemmatized Text:\", lemmatized_text)\n"
   ],
   "id": "fc49e9c8184fb5c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Frau Präsidentin, zur Geschäftsordnung.\n",
      "Lemmatized Text: Frau Präsidentin -- zu Geschäftsordnung --\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T20:34:10.887319Z",
     "start_time": "2024-05-02T20:34:10.885349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence = \"Frau Präsidentin, zur Geschäftsordnung.\"\n",
    "sentence = \"europäisch europäischen\"\n"
   ],
   "id": "d16f34e1a4edf687",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T20:34:11.671706Z",
     "start_time": "2024-05-02T20:34:11.664666Z"
    }
   },
   "cell_type": "code",
   "source": "doc = nlp(sentence)",
   "id": "2ade6e6d458a816f",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T20:34:11.824559Z",
     "start_time": "2024-05-02T20:34:11.806370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokens = []\n",
    "for token in doc:\n",
    "    tokens.append(token.lemma_)"
   ],
   "id": "63594607e28bdc6e",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T20:34:12.222106Z",
     "start_time": "2024-05-02T20:34:12.220187Z"
    }
   },
   "cell_type": "code",
   "source": "tokens",
   "id": "110c94ad70ed52f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['europäisch', 'europäisch']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T17:49:57.464110Z",
     "start_time": "2024-05-02T17:49:57.462391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#lemmatizer\n",
    "import spacy\n"
   ],
   "id": "77818020c40b5b92",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6addefa22497931c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
